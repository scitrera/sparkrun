model: Qwen/Qwen3-Coder-Next-FP8
runtime: sglang  # will handle additional logistics related to distributed inference
min_nodes: 2
container: scitrera/dgx-spark-sglang:0.5.8-t5

metadata:
  description: Qwen3 Coder Next (upstream FP8 quant) -- cluster only
  maintainer: scitrera.ai <open-source-team@scitrera.com>
  model_params: 80B
  model_dtype: fp8

defaults:
  port: 8000
  host: 0.0.0.0
  tensor_parallel: 2
  gpu_memory_utilization: 0.8
  max_model_len: 200000
  served_model_name: qwen3-coder-next
  attention_backend: triton  # NOTE: triton required for distributed sglang MoE here
  tool_call_parser: qwen3_coder
  fp8_gemm_backend: "cutlass"  # TODO: decide on cutlass or flashinfer_trtllm; NOTE: auto/deep_gemm won't work right now

command: |
  python3 -m sglang.launch_server \
      --model-path {model} \
      --served-model-name {served_model_name} \
      --context-length {max_model_len} \
      --mem-fraction-static {gpu_memory_utilization} \
      --tp-size {tensor_parallel} \
      --host {host} \
      --port {port} \
      --attention-backend {attention_backend} \
      --fp8-gemm-backend {fp8_gemm_backend} \
      --tool-call-parser {tool_call_parser}