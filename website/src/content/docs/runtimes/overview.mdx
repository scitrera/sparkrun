---
title: Runtimes Overview
description: Supported inference runtimes in sparkrun.
---

sparkrun supports multiple inference runtimes through a plugin system. Each runtime knows how to launch a specific inference engine, configure networking, and manage containers.

## Runtime comparison

| Runtime | Engine | Multi-Node | Use Case |
|---|---|---|---|
| `vllm` | vLLM | Yes (Ray) | Production inference, broad model support |
| `sglang` | SGLang | Yes (native) | High-throughput inference, structured generation, experimental GGUF |
| `llama-cpp` | llama.cpp | Experimental | GGUF quantized models, lightweight deployment |
| `eugr-vllm` | vLLM (eugr) | Yes (delegated) | Gold standard community vLLM — nightly builds, mods, battle-tested |

## How runtimes work

**Recipes** declare which runtime to use via the `runtime` field. The runtime handles:

- Container launch configuration (volumes, networking, GPU access)
- InfiniBand/RDMA detection and NCCL environment variables
- Multi-node clustering (Ray for vLLM, native for SGLang)
- Model pre-sync and cache path resolution
- Command generation when no explicit `command` template is provided

## Choosing a runtime

- **vLLM** is the default and most widely supported. Use it unless you have a specific reason to choose another.
- **SGLang** offers competitive performance, good structured generation support, and experimental GGUF quantized model serving.
- **llama.cpp** is ideal for GGUF quantized models or when you want a lightweight alternative.
- **eugr-vllm** is the gold standard for community-driven vLLM — nightly builds, mods, and a battle-tested container pipeline.

## Maintained container images

Scitrera maintains several specialized container images for DGX Spark. For more information on how these images are built and maintained, see the [cuda-containers](https://github.com/scitrera/cuda-containers) repository.

| Image | Description | Docker Hub |
|---|---|---|
| `dgx-spark-vllm` | Optimized vLLM for DGX Spark | [Link](https://hub.docker.com/r/scitrera/dgx-spark-vllm) |
| `dgx-spark-sglang` | Optimized SGLang for DGX Spark | [Link](https://hub.docker.com/r/scitrera/dgx-spark-sglang) |
| `dgx-spark-llama-cpp` | Optimized llama.cpp for DGX Spark | [Link](https://hub.docker.com/r/scitrera/dgx-spark-llama-cpp) |
| `dgx-spark-pytorch-runtime` | PyTorch runtime with Spark-optimized dependencies | [Link](https://hub.docker.com/r/scitrera/dgx-spark-pytorch-runtime) |
| `dgx-spark-pytorch-dev` | PyTorch development image for DGX Spark | [Link](https://hub.docker.com/r/scitrera/dgx-spark-pytorch-dev) |

We recommend checking Docker Hub for the latest available tags for each image.

Note: These are not the only images that work with sparkrun; sparkrun is designed for general use. The above images are maintained by Scitrera to ensure compatibility and performance on DGX Spark, but you can use any container that meets the requirements of your chosen runtime.
The images above are intended to be a middle ground. The NVIDIA NGC containers are often a bit far behind in the interest of stability, while bleeding-edge community images can sometimes be a bit too volatile for production use.
The above images are regularly updated to keep up with the latest versions of vLLM, SGLang, and llama.cpp while ensuring stability on DGX Spark.
