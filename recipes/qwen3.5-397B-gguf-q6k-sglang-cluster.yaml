model: unsloth/Qwen3.5-397B-A17B-GGUF:Q6_K
runtime: sglang  # will handle additional logistics related to distributed inference
min_nodes: 4
container: scitrera/dgx-spark-sglang:0.5.8-t5

metadata:
  description: Qwen3.5-397B-A17B (unsloth, Q6_K, GGUF)
  maintainer: scitrera.ai <open-source-team@scitrera.com>

defaults:
  port: 8000
  host: 0.0.0.0
  tensor_parallel: 4
  gpu_memory_utilization: 0.8
  max_model_len: 200000
  served_model_name: qwen3.5-397b
  attention_backend: triton  # NOTE: triton required for distributed sglang MoE here
  tool_call_parser: qwen3_coder
  tokenizer_path: Qwen/Qwen3.5-397B-A17B  # GGUF via sglang requires explicit hf tokenizer config

command: |
  python3 -m sglang.launch_server \
      --model-path {model} \
      --served-model-name {served_model_name} \
      --context-length {max_model_len} \
      --mem-fraction-static {gpu_memory_utilization} \
      --tp-size {tensor_parallel} \
      --host {host} \
      --port {port} \
      --attention-backend {attention_backend} \
      --tokenizer-path {tokenizer_path} \
      --tool-call-parser {tool_call_parser}
