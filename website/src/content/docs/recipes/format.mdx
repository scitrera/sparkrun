---
title: Recipe Format
description: Complete specification of the sparkrun recipe YAML format.
---

A sparkrun recipe is a YAML file that describes everything needed to launch an inference workload: the model, the container image, the runtime engine, default parameters, and the serve command. Recipes are the central abstraction in sparkrun — they let you capture a known-good configuration once and replay it with a single command.

```bash
sparkrun run my-recipe --solo          # use defaults
sparkrun run my-recipe -H host1,host2  # override hosts
sparkrun run my-recipe -o port=9000    # override any default
```

## Minimal recipe

The smallest useful recipe needs only `model`, `runtime`, `container`, and `command`:

```yaml
model: Qwen/Qwen3-1.7B
runtime: vllm
container: scitrera/dgx-spark-vllm:0.16.0-t5

defaults:
  port: 8000
  host: 0.0.0.0

command: |
  vllm serve {model} --host {host} --port {port}
```

## Full recipe example

```yaml
model: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-NVFP4
model_revision: abc123def
runtime: vllm
min_nodes: 1
container: scitrera/dgx-spark-vllm:0.16.0-t5

metadata:
  description: NVIDIA Nemotron 3 Nano 30B (upstream NVFP4) -- cluster or solo
  maintainer: scitrera.ai <open-source-team@scitrera.com>
  model_params: 30B
  model_dtype: nvfp4

defaults:
  port: 8000
  host: 0.0.0.0
  tensor_parallel: 1
  gpu_memory_utilization: 0.8
  max_model_len: 200000
  served_model_name: nemotron3-30b-a3b
  tool_call_parser: qwen3_coder

env:
  VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"

command: |
  vllm serve \
      {model} \
      --served-model-name {served_model_name} \
      --max-model-len {max_model_len} \
      --gpu-memory-utilization {gpu_memory_utilization} \
      -tp {tensor_parallel} \
      --host {host} \
      --port {port} \
      --enable-auto-tool-choice \
      --tool-call-parser {tool_call_parser} \
      --trust-remote-code
```

## Core fields

### `model` (required)

The HuggingFace model identifier to serve.

```yaml
# Standard HuggingFace model
model: Qwen/Qwen3-1.7B

# GGUF model with quantization variant (llama-cpp runtime)
model: Qwen/Qwen3-1.7B-GGUF:Q8_0
```

For GGUF models, the colon syntax (`repo:quant`) selects a specific quantization variant. sparkrun uses this to download only the matching quant files rather than the entire repository.

The model value is injected into the command template as `{model}` and is also used for model pre-sync (downloading and distributing weights to target hosts before launch).

### `model_revision`

Pin the model to a specific HuggingFace revision (branch, tag, or commit hash).

```yaml
model: QuantTrio/MiniMax-M2.5-AWQ
model_revision: bbe738792c
```

This affects model download, cache checking, VRAM estimation, and model sync. Use it for reproducible deployments — without it, a model's `main` branch may change between downloads.

### `runtime` (required)

Which inference engine to use:

| Value | Engine | Clustering | Notes |
|---|---|---|---|
| `vllm` | vLLM | Ray | First-class. Solo and multi-node. |
| `sglang` | SGLang | Native | First-class. Solo and multi-node. |
| `llama-cpp` | llama.cpp | N/A | Solo mode. GGUF models. |
| `eugr-vllm` | vLLM (eugr) | Delegated | Compatibility runtime. |

Defaults to `vllm` if omitted.

### `container` (required)

The Docker/OCI container image to run.

```yaml
container: scitrera/dgx-spark-vllm:0.16.0-t5
container: scitrera/dgx-spark-sglang:0.5.8-t5
container: scitrera/dgx-spark-llama-cpp:b8076-cu131
```

### `command` (recommended)

The shell command to execute inside the container. Uses `{placeholder}` syntax — any key from `defaults` (or CLI overrides) can be referenced.

```yaml
command: |
  vllm serve \
      {model} \
      --served-model-name {served_model_name} \
      --host {host} \
      --port {port}
```

Substitution is iterative: if a placeholder resolves to a string containing another `{placeholder}`, it will be expanded in a second pass.

## Topology fields

### `mode`

Explicit topology mode. Usually not needed — sparkrun infers from `min_nodes` and `max_nodes`.

| Value | Meaning |
|---|---|
| `auto` | (default) sparkrun decides based on node counts and CLI flags |
| `solo` | Forces single-node. Sets `min_nodes = max_nodes = 1`. |
| `cluster` | Forces multi-node. Requires 2+ hosts. |

### `min_nodes`

Minimum number of nodes required. Defaults to `1`.

On DGX Spark, each node has one GPU, so `min_nodes` is effectively the minimum GPU count.

### `max_nodes`

Maximum number of nodes supported. Defaults to unlimited.

Set `max_nodes: 1` for models or runtimes that do not support multi-node inference.

### `solo_only` / `cluster_only`

Boolean shorthand flags:

```yaml
solo_only: true     # equivalent to max_nodes: 1, mode: solo
cluster_only: true  # equivalent to min_nodes: 2, mode: cluster
```

## Configuration fields

### `defaults`

A flat dictionary of default parameter values. Every key is available as a `{placeholder}` in the command template and can be overridden at launch via CLI flags or `-o key=value`.

```yaml
defaults:
  port: 8000
  host: 0.0.0.0
  tensor_parallel: 1
  gpu_memory_utilization: 0.8
  max_model_len: 200000
  served_model_name: nemotron3-30b-a3b
```

Well-known keys with dedicated CLI flags:

| Default key | CLI flag | Description |
|---|---|---|
| `port` | `--port` | Serve port |
| `tensor_parallel` | `--tp` | Tensor parallelism degree |
| `gpu_memory_utilization` | `--gpu-mem` | GPU memory fraction (0.0–1.0) |

**Config chain precedence** (highest wins):
1. CLI overrides (`--port 9000`, `-o key=value`)
2. Recipe defaults

### `env`

Environment variables injected into the container.

```yaml
env:
  VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
  HF_TOKEN: "${HF_TOKEN}"
```

Shell variable references (`${VAR}`) are expanded from the **control machine's** environment when the recipe is loaded — forward secrets without hardcoding them.

## Metadata fields

### `metadata`

Descriptive and VRAM-estimation fields. Does not affect how the workload runs.

```yaml
metadata:
  description: NVIDIA Nemotron 3 Nano 30B (upstream NVFP4)
  maintainer: scitrera.ai <open-source-team@scitrera.com>
  model_params: 30B
  model_dtype: nvfp4
```

#### VRAM estimation fields

| Field | Type | Description |
|---|---|---|
| `model_params` | string | Parameter count: `"1.7B"`, `"30B"`, `"397B"` |
| `model_dtype` | string | Weight dtype: `bf16`, `fp16`, `fp8`, `nvfp4`, `int4`, `q8_0`, etc. |
| `kv_dtype` | string | KV cache dtype (defaults to `bfloat16`) |
| `num_layers` | int | Number of transformer layers |
| `num_kv_heads` | int | Number of key-value attention heads |
| `head_dim` | int | Dimension per attention head |
| `model_vram` | float | Override: total model weight VRAM in GB |
| `kv_vram_per_token` | float | Override: KV cache bytes per token |

sparkrun auto-detects most of these from HuggingFace, but metadata values take precedence — useful for quantized models.

## Runtime-specific fields

### `runtime_config`

A dictionary for runtime-specific configuration, primarily used by the `eugr-vllm` runtime:

```yaml
runtime_config:
  mods: [my-custom-mod]
  build_args: [--some-flag]
```

Unknown top-level keys are automatically swept into `runtime_config`.

## Recipe discovery

sparkrun searches for recipes in this order:
1. **Exact/relative file path** — if the argument is an existing file
2. **Bundled recipes** — shipped with sparkrun
3. **Registry paths** — from configured registries
4. **Registry stem matching** — searches by filename stem

Filenames are matched with or without `.yaml`/`.yml` extensions.

## Command template substitution

The `command` field supports `{placeholder}` substitution from the config chain:

```yaml
defaults:
  port: 8000
  served_model_name: my-model

command: |
  vllm serve {model} --port {port} --served-model-name {served_model_name}
```

With `sparkrun run recipe -o port=9000`, this renders as:
```
vllm serve Qwen/Qwen3-1.7B --port 9000 --served-model-name my-model
```

The special placeholder `{model}` is always available from the top-level `model` field.
