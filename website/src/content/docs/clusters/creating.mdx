---
title: Creating Clusters
description: Create and configure named clusters for DGX Spark hosts.
---

Clusters are named groups of DGX Spark hosts. Save your hosts once and reference them by name in every command.

## Create a cluster

```bash
sparkrun cluster create mylab \
  --hosts 192.168.11.13,192.168.11.14 \
  -d "My DGX Spark lab"
```

### Options

| Option | Description |
|---|---|
| `--hosts` / `-H` | Comma-separated list of host IPs or names |
| `-d` / `--description` | Human-readable description |
| `--user` | SSH user for connecting to hosts |

## Host ordering

The first host in a cluster definition is used as the **head node** for multi-node jobs. Order the remaining hosts however you like â€” they become workers.

```bash
# 192.168.11.13 is the head node
sparkrun cluster create mylab --hosts 192.168.11.13,192.168.11.14
```

## SSH user

By default, sparkrun uses your current OS user for SSH. Set a per-cluster user:

```bash
sparkrun cluster create mylab \
  --hosts 192.168.11.13,192.168.11.14 \
  --user dgxuser
```

Or override per-command with `--user`.

## Update a cluster

Modify hosts, description, or SSH user of an existing cluster:

```bash
sparkrun cluster update mylab --hosts 192.168.11.13,192.168.11.14,192.168.11.15
sparkrun cluster update mylab --user newuser
sparkrun cluster update mylab -d "Updated description"
```

## Set as default

```bash
sparkrun cluster set-default mylab
```

When a default cluster is set, commands like `sparkrun run` use it automatically without needing `--cluster` or `--hosts`.

## Next steps

After creating a cluster, set up [SSH access](/getting-started/ssh-setup/) for multi-node inference.
